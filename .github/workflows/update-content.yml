name: Daily Content Update

on:
  schedule:
    # Runs at 2 AM UTC (adjust as needed)
    - cron: '0 2 * * *'
  workflow_dispatch:  # Allows manual triggering for testing

jobs:
  update-content:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests feedparser beautifulsoup4
          
      - name: Generate new content
        run: |
          python - <<EOF
          import json
          import random
          import feedparser
          import requests
          from bs4 import BeautifulSoup
          from datetime import datetime
          
          # Your 12 categories with RSS feeds
          RSS_FEEDS = {
              "Investing & Retirement": [
                  "https://www.investopedia.com/rss/articles",
                  "https://www.morningstar.com/rss/articles",
              ],
              "Real Estate & Home Ownership": [
                  "https://www.zillow.com/blog/feed/",
                  "https://www.realtor.com/feeds/advice",
              ],
              "Fantasy Baseball": [
                  "https://www.fantasypros.com/mlb/rss/articles.php",
                  "https://www.rotoballer.com/feed/mlb",
              ],
              "Health & Fitness": [
                  "https://www.healthline.com/nutrition/feed",
                  "https://www.self.com/feed/rss",
              ],
              "Productivity & Task Management": [
                  "https://lifehacker.com/productivity/rss",
                  "https://todoist.com/blog/feed",
              ],
              "Automation & Scripts": [
                  "https://zapier.com/blog/feeds/latest/",
                  "https://www.automate.io/blog/feed/",
              ],
              "Excel & Power BI Tips": [
                  "https://exceljet.net/rss.xml",
                  "https://powerbi.microsoft.com/en-us/blog/feed/",
              ],
              "Learning & Personal Growth": [
                  "https://hbr.org/topics/developing-yourself/feed",
                  "https://jamesclear.com/feed",
              ],
              "Prompt Engineering": [
                  "https://arxiv.org/rss/cs.CL",
                  "https://www.reddit.com/r/PromptEngineering/.rss",
              ],
              "Tech & Tools": [
                  "https://www.wired.com/feed/category/gear/latest/rss",
                  "https://lifehacker.com/tech/rss",
              ],
              "Writing & Communication": [
                  "https://www.grammarly.com/blog/feed/",
                  "https://blog.hubspot.com/marketing/rss.xml",
              ],
              "Travel - Europe Highlights & Tips": [
                  "https://www.lonelyplanet.com/blog/feed/europe",
                  "https://www.ricksteves.com/feed",
              ],
          }
          
          # For beginner-intermediate content
          BEGINNER_KEYWORDS = [
              "beginner", "start", "basic", "fundamental", "introduction", "101", 
              "how to", "guide", "simple", "easy", "first", "learn", "tutorial"
          ]
          
          def get_article_from_feed(feed_urls):
              """Get a random article from one of the feeds, preferably beginner-friendly"""
              beginner_articles = []
              all_articles = []
              
              for feed_url in feed_urls:
                  try:
                      # Parse the RSS feed
                      feed = feedparser.parse(feed_url)
                      
                      # Check each entry in the feed
                      for entry in feed.entries[:10]:  # Check only the 10 most recent entries
                          title = entry.title.lower()
                          description = entry.get('description', '').lower()
                          
                          article = {
                              'title': entry.title,
                              'url': entry.link,
                              'summary': entry.get('description', '')[:150] + '...' if entry.get('description') else "Explore this resource to expand your knowledge."
                          }
                          
                          # Clean up summary by removing HTML tags
                          soup = BeautifulSoup(article['summary'], 'html.parser')
                          article['summary'] = soup.get_text()
                          
                          # Check if this looks like beginner content
                          if any(keyword in title or keyword in description for keyword in BEGINNER_KEYWORDS):
                              beginner_articles.append(article)
                          
                          all_articles.append(article)
                  
                  except Exception as e:
                      print(f"Error processing feed {feed_url}: {e}")
                      continue
              
              # Prefer beginner articles, but fall back to any article if needed
              if beginner_articles:
                  return random.choice(beginner_articles)
              elif all_articles:
                  return random.choice(all_articles)
              else:
                  # Fallback content if feeds fail
                  return {
                      'title': f"Daily Resource for {datetime.now().strftime('%Y-%m-%d')}",
                      'url': "https://example.com",
                      'summary': "Our systems are gathering fresh content. Check back tomorrow for new resources!"
                  }
          
          def main():
              daily_links = []
              
              # Generate one link for each category
              for category, feeds in RSS_FEEDS.items():
                  article = get_article_from_feed(feeds)
                  
                  link_data = {
                      'category': category,
                      'title': article['title'],
                      'url': article['url'],
                      'summary': article['summary']
                  }
                  
                  daily_links.append(link_data)
              
              # Write to JSON file
              with open('curiosity-boost-today.json', 'w') as f:
                  json.dump(daily_links, f, indent=2)
              
              print(f"Generated {len(daily_links)} links for {datetime.now().strftime('%Y-%m-%d')}")
          
          if __name__ == "__main__":
              main()
          EOF
          
      - name: Create JSON file if it doesn't exist
        run: |
          if [ ! -f curiosity-boost-today.json ]; then
            echo "[]" > curiosity-boost-today.json
          fi
          
      - name: Commit and push changes
        run: |
          git config --global user.name 'GitHub Action'
          git config --global user.email 'action@github.com'
          git add curiosity-boost-today.json
          git diff --quiet && git diff --staged --quiet || git commit -m "Update daily content for $(date +'%Y-%m-%d')"
          git push
